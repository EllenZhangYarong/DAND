{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]\n",
      "0.0\n",
      "svmem(total=17117237248, available=10399232000, percent=39.2, used=6718005248, free=10399232000)\n",
      "memory GB: 0.13238143920898438\n"
     ]
    }
   ],
   "source": [
    "# By @smth \n",
    "import sys\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()\n",
    "memReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Transform our Data\n",
    "\n",
    "We'll be using PyTorch's [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) class which makes is very easy to load data from a directory. For example, the training images are all stored in a directory path that looks like this:\n",
    "```\n",
    "root/class_1/xxx.png\n",
    "root/class_1/xxy.png\n",
    "root/class_1/xxz.png\n",
    "\n",
    "root/class_2/123.png\n",
    "root/class_2/nsdf3.png\n",
    "root/class_2/asd932_.png\n",
    "```\n",
    "\n",
    "Where, in this case, the root folder for training is `flower_photos/train/` and the classes are the names of flower types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and test data directories\n",
    "data_dir = 'flower_data/'\n",
    "train_dir = os.path.join(data_dir, 'train/')\n",
    "valid_dir = os.path.join(data_dir, 'valid/')\n",
    "test_dir = os.path.join(data_dir, 'google_test_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes are folders in each directory with these names\n",
    "\n",
    "import json\n",
    "\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Data\n",
    "\n",
    "When we perform transfer learning, we have to shape our input data into the shape that the pre-trained model expects. VGG16 expects `224`-dim square images as input and so, we resize each flower image to fit this mold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training images:  6552\n",
      "Num test images:  818\n"
     ]
    }
   ],
   "source": [
    "# load and transform data using ImageFolder\n",
    "\n",
    "# VGG-16 Takes 224x224 images as input, so we resize all of them\n",
    "# data_transform = transforms.Compose([transforms.RandomResizedCrop(224), \n",
    "#                                       transforms.ToTensor()])\n",
    "\n",
    "data_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
    "                    transforms.RandomRotation(10),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                    ])\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=data_transforms)\n",
    "valid_data = datasets.ImageFolder(valid_dir, transform=data_transforms)\n",
    "\n",
    "# print out some data stats\n",
    "print('Num training images: ', len(train_data))\n",
    "print('Num test images: ', len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders and Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader parameters\n",
    "batch_size = 20\n",
    "num_workers=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-16511ecad9b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_to_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: '0'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some sample data\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "np.clip(images, 0, 1)\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(cat_to_name[str(int(labels[idx]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the Model\n",
    "\n",
    "To define a model for training we'll follow these steps:\n",
    "1. Load in a pre-trained VGG16 model\n",
    "2. \"Freeze\" all the parameters, so the net acts as a fixed feature extractor \n",
    "3. Remove the last layer\n",
    "4. Replace the last layer with a linear classifier of our own\n",
    "\n",
    "**Freezing simply means that the parameters in the pre-trained model will *not* change during training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model from pytorch\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# print out the model structure\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.classifier[6].in_features) \n",
    "print(vgg16.classifier[6].out_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze training for all \"features\" layers\n",
    "for param in vgg16.features.parameters():\n",
    "    param.require_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Final Classifier Layer\n",
    "\n",
    "Once you have the pre-trained feature extractor, you just need to modify and/or add to the final, fully-connected classifier layers. In this case, we suggest that you repace the last layer in the vgg classifier group of layers. \n",
    "> This layer should see as input the number of features produced by the portion of the network that you are not changing, and produce an appropriate number of outputs for the flower classification task.\n",
    "\n",
    "You can access any layer in a pretrained network by name and (sometimes) number, i.e. `vgg16.classifier[6]` is the sixth layer in a group of layers named \"classifier\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "n_inputs = vgg16.classifier[6].in_features\n",
    "\n",
    "# add last linear layer (n_inputs -> 5 flower classes)\n",
    "# new layers automatically have requires_grad = True\n",
    "last_layer = nn.Linear(n_inputs,len(cat_to_name))\n",
    "\n",
    "vgg16.classifier[6] = last_layer\n",
    "\n",
    "# if GPU is available, move the model to GPU\n",
    "if train_on_gpu:\n",
    "    vgg16.cuda()\n",
    "\n",
    "# check to see that your last layer produces the expected number of outputs\n",
    "print(vgg16.classifier[6].out_features)\n",
    "#print(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Specify [Loss Function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "Below we'll use cross-entropy loss and stochastic gradient descent with a small learning rate. Note that the optimizer accepts as input _only_ the trainable parameters `vgg.classifier.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.001\n",
    "# optimizer = optim.SGD(vgg16.classifier.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(vgg16.classifier.parameters(), lr=0.00001, amsgrad=True, eps=1e-8, weight_decay=1e-5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "Here, we'll train the network.\n",
    "\n",
    "> **Exercise:** So far we've been providing the training code for you. Here, I'm going to give you a bit more of a challenge and have you write the code to train the network. Of course, you'll be able to see my solution if you need help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model,filename):\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading model '{}'\".format(filename))\n",
    "        model.load_state_dict(torch.load(filename))\n",
    "        print(\"=> loaded model from '{}'\".format(filename))\n",
    "    else:\n",
    "        print(\"=> no model found at '{}'\".format(filename))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading model 'models/model_flower_vgg_3.pt'\n",
      "=> loaded model from 'models/model_flower_vgg_3.pt'\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint(vgg16, 'models/model_flower_vgg_3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing\n",
    "\n",
    "Below you see the test accuracy for each flower class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                    ])\n",
    "\n",
    "test_data = datasets.ImageFolder(test_dir, transform=data_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\PIL\\Image.py:2655: UserWarning: image file could not be identified because WEBP support not installed\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "cannot identify image file <_io.BufferedReader name='flower_data/google_test_data/36\\\\5. single-crimson-cattleya-rubylipped-labiata-260nw-556853779.jpg'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-098b19d99a2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# iterate over test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m# move tensors to GPU if CUDA is available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrain_on_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \"\"\"\n\u001b[0;32m    100\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2655\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2656\u001b[0m     raise IOError(\"cannot identify image file %r\"\n\u001b[1;32m-> 2657\u001b[1;33m                   % (filename if filename else fp))\n\u001b[0m\u001b[0;32m   2658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2659\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: cannot identify image file <_io.BufferedReader name='flower_data/google_test_data/36\\\\5. single-crimson-cattleya-rubylipped-labiata-260nw-556853779.jpg'>"
     ]
    }
   ],
   "source": [
    "# track test loss \n",
    "# over 5 flower classes\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(102))\n",
    "class_total = list(0. for i in range(102))\n",
    "\n",
    "model.eval() # eval mode\n",
    "\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update  test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(len(target.data)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate avg test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(len(class_total)):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %.2f%% (%2d/%2d)' % (\n",
    "            cat_to_name[str(i+1)], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %.2f%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualize Sample Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 61.25 MiB (GPU 0; 8.00 GiB total capacity; 6.07 GiB already allocated; 25.17 MiB free; 176.35 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-cb79bbc3317b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# get sample outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m# convert output probabilities to predicted class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\torchvision\\models\\vgg.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\evforpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 61.25 MiB (GPU 0; 8.00 GiB total capacity; 6.07 GiB already allocated; 25.17 MiB free; 176.35 MiB cached)"
     ]
    }
   ],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images_ = images.numpy()\n",
    "\n",
    "# move model inputs to cuda, if GPU available\n",
    "if train_on_gpu:\n",
    "    images = images.cuda()\n",
    "\n",
    "# get sample outputs\n",
    "output = model(images)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds_tensor = torch.max(output, 1)\n",
    "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images_[idx], (1, 2, 0)))\n",
    "    ax.set_title(\"{} ({})\".format( cat_to_name[str(int(preds[idx])+1)], cat_to_name[str(int(labels[idx])+1)]),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
